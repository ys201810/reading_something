# [Large Multimodal Agents: A Survey](https://arxiv.org/pdf/2402.15116.pdf)
簡易まとめ：自分の言葉でまとめたもの。  
GPT翻訳：GPTで翻訳した全文。  

## 簡易まとめ
### abstract
外部情報の検索を用いた、思考連鎖の反復的な改良が、大規模言語モデルでの長期生成タスクの推論と生成能力を大幅に向上させるが、幻覚の発生が課題である。  
提案するRATでは、ゼロショットで生成したCoTを、タスククエリや現在および過去の思考ステップに関連する情報用いて、一つ一つの思考ステップを改訂できる。  
RATをGPT-3.5、GPT-4、そしてCodeLLaMA-7bに適応することで、  
コード生成で平均13.63％、数学的推論で16.96％、創造的なライティングで19.2％、身体的タスク計画で42.78％の評価スコアが相対的に増加した。デモは[ここ](https://craftjarvis.github.io/RAT)。  

### 1. introduction
LLMは、様々な自然言語推論タスクにおいて大きな進歩を遂げている。  
特に、高度なプロンプト戦略、思考の連鎖（CoT）プロンプトを組み合わせた場合に上手くいっている。  
同時に、LLMの推論の正確性について課題感も増えている。  
具体的には、モデルの応答や中間推論パス(CoTの連鎖中)における幻覚である。  
この課題は、ゼロショットCoTプロンプト(ステップ・バイ・ステップで考えよう)や、複数ステップでの処理や、文脈認識推論が必要な長期生成タスクで重要である。  

この課題に対し、様々なプロンプト技術が提案されている。  
有望なのはRAGで、RAGでは外部データを取得して、より事実に基づいた推論を可能にする。  
本論では、このRAGを高度な長期推論とどのように利用すると良いかを探求する。  
筆者らは、中間推論時の幻覚は、外部の知識の利用により軽減できると考えている。  
図1に本論の手法のイメージを示す。  

図1: RATパイプライン  
![図1](../../../data/27.png)

step0(図1の左)の流れは、ユーザーが質問を生成・LLMが回答のためのタスクをゼロショットで分解・分解したタスクを順番づけて実行する。  
step1~n(図1の右)の流れは、step0で生成されたnステップの回数、実施される。  
■ step1の流れ  
【Retrieval】Cotで作られた最初のタスクT1を用いて、関連する情報R1が検索される。  
【Augmented Revision】I(元のユーザープロンプト)と、R1(関連情報)を使って $T_1^\*$ を生成する。  
【Revised CoTs】 $T_1^\*$ と元のCoTを組み合わせて、新しいCoTを生成する。  
■ stepNの流れ
【Retrieval】改良した $T_1^\*$ ~ $T_{n-1}^\*$ と $T_n$ を用いて、関連する情報 $R_n$ を検索する。  
【Augmented Revision】改良した $T_1^\*$ ~ $T_{n-1}^\*$ と、I(元のユーザープロンプト)と、 $R_n$ (関連情報)を使って $T_n^\*$ を生成する。  
【Revised CoTs】 $T_1^\*$ ~ $T_{n}^\*$ で新しいCoTを生成する。  


※ Augmented Revisionのstep1で、 $T_1^\*$ が入力になっているように見えるけど、多分ミスだと思う。  
n-1までの $T_{n-1}^\*$ が入力になるはず。なぜなら、Revisionが目的なのに、 $T_1^\*$ が分かっているならこれをする必要がないから。  



私たちの戦略は2つの主要なアイデアを含みます。  
まず、LLMによって生成された初期のゼロショットCoTと元のタスクプロンプトを使用して、おそらく欠陥のあるCoTを改訂するのに役立つ情報を取得します。  
次に、CoT全体を取得して一度に最終的な応答を生成するのではなく、LLMがCoT（一連のサブタスク）に従ってステップバイステップで応答を生成し、  
現在の思考ステップのみがタスクプロンプト、現在および過去のCoTで取得した情報に基づいて改訂される段階的なアプローチを考案します。  
この戦略は、複雑な長期問題解決中に外部知識を活用して段階的に思考を調整する人間の推論プロセスに例えることができます。  
RATとその対象との比較は図2で見ることができます。  

図2: 様々なLLMでの回答生成の比較  
![図1](../../../data/28.png)

RATを、コード生成、数学的推論、身体的タスク計画、創造的ライティングを含む幅広い困難な長期タスクに適用して評価しました。  
さまざまな規模の複数のLLMを使用します：GPT-3.5、GPT-4、CodeLLaMA7b。  
結果は、これらのLLMにRATを組み合わせることが、標準のCoTプロンプトやRAGアプローチよりも大きな利点をもたらすことを示しています。  
特に、私たちが選択したタスク群全体で新たな最先端のパフォーマンスレベルを観察します：  
1) コード生成では、HumanEval（+20.94%）、HumanEval+（+18.89%）、MBPP（+14.83%）、MBPP+（+1.86%)
2) 数学的推論問題では、GSM8K（+8.36%）、GSMHard（+31.37%）
3) Minecraftタスク計画（実行可能性で2.96倍、妥当性で+51.94%）
4) 創造的ライティング（人間のスコアで+19.19%）

追加の削除研究は、RATの二つの主要な要素、すなわちRAGを使用したCoTの改訂と進行的な改訂および生成が果たす重要な役割をさらに確認します。  
この研究は、人間が行うように、外部の知識の助けを借りて、LLMがゼロショット方式で推論プロセスをどのように改訂するかを明らかにしています。  





## GPT翻訳
### abstract
「情報検索を用いた思考連鎖の反復的な改訂が、長期生成タスクにおける大規模言語モデルの推論と生成能力を大幅に向上させる一方で、幻覚の発生を大幅に軽減することを探求します。  
特に、提案された方法であるリトリーバル拡張思考（RAT）は、ゼロショットCoTが生成された後、タスククエリ、現在および過去の思考ステップに関連する情報を検索して、  
一つ一つの思考ステップを改訂します。  
RATをGPT-3.5、GPT-4、そしてCodeLLaMA-7bに適用することで、これらのモデルの長期生成タスクにおけるパフォーマンスが顕著に向上し、  
コード生成で平均13.63％、数学的推論で16.96％、創造的なライティングで19.2％、身体的タスク計画で42.78％の評価スコアが相対的に増加しました。  
デモページは https://craftjarvis.github.io/RAT で見ることができます。  

### 1. introduction
大規模言語モデル（LLM）は、様々な自然言語推論タスクにおいて実り多い進歩を遂げています、特に大規模モデルと高度なプロンプト戦略、  
特に思考の連鎖（CoT）プロンプトを組み合わせた場合です。  
しかし、LLMの推論の事実的な正確性についての懸念が増えています。  
これは、モデルの応答や中間推論パス、すなわちCoTにおける可能な幻覚を引用しています。  
この問題は、ゼロショットCoTプロンプト、いわゆる「ステップ・バイ・ステップで考えよう」や、  
複数ステップおよび文脈認識推論が必要な長期生成タスク（コード生成、タスク計画、数学的推論など）においてさらに重要になります。  
事実上有効な中間思考は、これらのタスクの成功完了には重要です。  

この問題を軽減するために、いくつかのプロンプト技術が提案されています。  
その中で有望な方向性であるリトリーバル拡張生成（RAG）は、人間の推論からの洞察を求め、より事実に基づいた推論を促進するために取得した情報を活用します。  
本論文では、RAGを高度な長期推論とどのように相乗効果を発揮させるかを探求します。  
私たちの直感では、中間推論過程内の幻覚は外部の知識の助けによって軽減される可能性があります。  
結果として提示されるプロンプト戦略、リトリーバル拡張思考（RAT）は図1で示されています。  

図1: RATパイプライン  
![図1](../../../data/27.png)


私たちの戦略は2つの主要なアイデアを含みます。  
まず、LLMによって生成された初期のゼロショットCoTと元のタスクプロンプトを使用して、おそらく欠陥のあるCoTを改訂するのに役立つ情報を取得します。  
次に、CoT全体を取得して一度に最終的な応答を生成するのではなく、LLMがCoT（一連のサブタスク）に従ってステップバイステップで応答を生成し、  
現在の思考ステップのみがタスクプロンプト、現在および過去のCoTで取得した情報に基づいて改訂される段階的なアプローチを考案します。  
この戦略は、複雑な長期問題解決中に外部知識を活用して段階的に思考を調整する人間の推論プロセスに例えることができます。  
RATとその対象との比較は図2で見ることができます。  

図2: 様々なLLMでの回答生成の比較  
![図1](../../../data/28.png)

RATを、コード生成、数学的推論、身体的タスク計画、創造的ライティングを含む幅広い困難な長期タスクに適用して評価しました。  
さまざまな規模の複数のLLMを使用します：GPT-3.5、GPT-4、CodeLLaMA7b。  
結果は、これらのLLMにRATを組み合わせることが、標準のCoTプロンプトやRAGアプローチよりも大きな利点をもたらすことを示しています。  
特に、私たちが選択したタスク群全体で新たな最先端のパフォーマンスレベルを観察します：  
1) コード生成では、HumanEval（+20.94%）、HumanEval+（+18.89%）、MBPP（+14.83%）、MBPP+（+1.86%)
2) 数学的推論問題では、GSM8K（+8.36%）、GSMHard（+31.37%）
3) Minecraftタスク計画（実行可能性で2.96倍、妥当性で+51.94%）
4) 創造的ライティング（人間のスコアで+19.19%）

追加の削除研究は、RATの二つの主要な要素、すなわちRAGを使用したCoTの改訂と進行的な改訂および生成が果たす重要な役割をさらに確認します。  
この研究は、人間が行うように、外部の知識の助けを借りて、LLMがゼロショット方式で推論プロセスをどのように改訂するかを明らかにしています。  

### 2. Retrieval Augmented Thoughts
LLMを使用する際に、長期的な推論と生成をサポートし、幻覚を軽減することが私たちの目標です。  
長期的なタスクに満足のいくパフォーマンスを得るためには、二つの要素が不可欠です。  
まず、事実情報へのアクセスは、検索によって容易にされることができます。  
次に、複雑なタスクを完了するためのスクラッチパッドを概説する適切な中間ステップは、CoTによって容易にされることができます。  
しかし、これら二つを単純に組み合わせるだけでは必ずしも改善につながるわけではありません。  
依然として二つの疑問が残ります：(1) どのような情報が検索に関連するか、(2) 関連する事実情報を用いて推論ステップをどのように効果的に修正するか。  
この二つの疑問にどのように対処できるかをより深く理解するために、まずRAGとCoTの簡単な導入を提供します。  

























